{"id": "2511.04685", "categories": ["cs.AI", "math.OC", "90-04", "F.2.2"], "pdf": "https://arxiv.org/pdf/2511.04685", "abs": "https://arxiv.org/abs/2511.04685", "authors": ["Daniela Guericke", "Rolf van der Hulst", "Asal Karimpour", "Ieke Schrader", "Matthias Walter"], "title": "A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024", "comment": "23 pages, 2 figures, 10 tables", "summary": "We report about the algorithm, implementation and results submitted to the\nIntegrated Healthcare Timetabling Competition 2024 by Team Twente, which scored\nthird in the competition. Our approach combines mixed-integer programming,\nconstraint programming and simulated annealing in a 3-phase solution approach\nbased on decomposition into subproblems. Next to describing our approach and\ndescribing our design decisions, we share our insights and, for the first time,\nlower bounds on the optimal solution values for the benchmark instances. We\nfinally highlight open problems for which we think that addressing them could\nimprove our approach even further.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04855", "abs": "https://arxiv.org/abs/2511.04855", "authors": ["Vojtech Franc", "Jakub Paplham"], "title": "Epistemic Reject Option Prediction", "comment": null, "summary": "In high-stakes applications, predictive models must not only produce accurate\npredictions but also quantify and communicate their uncertainty. Reject-option\nprediction addresses this by allowing the model to abstain when prediction\nuncertainty is high. Traditional reject-option approaches focus solely on\naleatoric uncertainty, an assumption valid only when large training data makes\nthe epistemic uncertainty negligible. However, in many practical scenarios,\nlimited data makes this assumption unrealistic. This paper introduces the\nepistemic reject-option predictor, which abstains in regions of high epistemic\nuncertainty caused by insufficient data. Building on Bayesian learning, we\nredefine the optimal predictor as the one that minimizes expected regret -- the\nperformance gap between the learned model and the Bayes-optimal predictor with\nfull knowledge of the data distribution. The model abstains when the regret for\na given input exceeds a specified rejection cost. To our knowledge, this is the\nfirst principled framework that enables learning predictors capable of\nidentifying inputs for which the training data is insufficient to make reliable\ndecisions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04880", "abs": "https://arxiv.org/abs/2511.04880", "authors": ["Yu Bai", "Yukai Miao", "Dawei Wang", "Li Chen", "Fei Long", "Rundi Zhai", "Dan Li", "Yanyu Ren", "Tianfeng Liu", "Hongtao Xie", "Ce Yang", "Xuhui Cai"], "title": "DMA: Online RAG Alignment with Human Feedback", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems often rely on static retrieval,\nlimiting adaptation to evolving intent and content drift. We introduce Dynamic\nMemory Alignment (DMA), an online learning framework that systematically\nincorporates multi-granularity human feedback to align ranking in interactive\nsettings. DMA organizes document-, list-, and response-level signals into a\ncoherent learning pipeline: supervised training for pointwise and listwise\nrankers, policy optimization driven by response-level preferences, and\nknowledge distillation into a lightweight scorer for low-latency serving.\nThroughout this paper, memory refers to the model's working memory, which is\nthe entire context visible to the LLM for In-Context Learning.\n  We adopt a dual-track evaluation protocol mirroring deployment: (i)\nlarge-scale online A/B ablations to isolate the utility of each feedback\nsource, and (ii) few-shot offline tests on knowledge-intensive benchmarks.\nOnline, a multi-month industrial deployment further shows substantial\nimprovements in human engagement. Offline, DMA preserves competitive\nfoundational retrieval while yielding notable gains on conversational QA\n(TriviaQA, HotpotQA). Taken together, these results position DMA as a\nprincipled approach to feedback-driven, real-time adaptation in RAG without\nsacrificing baseline capability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04898", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04898", "abs": "https://arxiv.org/abs/2511.04898", "authors": ["Yule Wen", "Yixin Ye", "Yanzhe Zhang", "Diyi Yang", "Hao Zhu"], "title": "Real-Time Reasoning Agents in Evolving Environments", "comment": "30 pages", "summary": "Agents in the real world must make not only logical but also timely\njudgments. This requires continuous awareness of the dynamic environment:\nhazards emerge, opportunities arise, and other agents act, while the agent's\nreasoning is still unfolding. Despite advances in language model reasoning,\nexisting approaches fail to account for this dynamic nature. We introduce\nreal-time reasoning as a new problem formulation for agents in evolving\nenvironments and build Real-Time Reasoning Gym to demonstrate it. We study two\nparadigms for deploying language models in agents: (1) reactive agents, which\nemploy language models with bounded reasoning computation for rapid responses,\nand (2) planning agents, which allow extended reasoning computation for complex\nproblems. Our experiments show that even state-of-the-art models struggle with\nmaking logical and timely judgments in either paradigm. To address this\nlimitation, we propose AgileThinker, which simultaneously engages both\nreasoning paradigms. AgileThinker consistently outperforms agents engaging only\none reasoning paradigm as the task difficulty and time pressure rise,\neffectively balancing reasoning depth and response latency. Our work\nestablishes real-time reasoning as a critical testbed for developing practical\nagents and provides a foundation for research in temporally constrained AI\nsystems, highlighting a path toward real-time capable agents.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05269", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05269", "abs": "https://arxiv.org/abs/2511.05269", "authors": ["Ishan Kavathekar", "Hemang Jain", "Ameya Rathod", "Ponnurangam Kumaraguru", "Tanuja Ganu"], "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems", "comment": "Accepted at ICML 2025 MAS Workshop. This version includes additional\n  experiments and analysis", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04707", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04707", "abs": "https://arxiv.org/abs/2511.04707", "authors": ["Rishi Rajesh Shah", "Chen Henry Wu", "Shashwat Saxena", "Ziqian Zhong", "Alexander Robey", "Aditi Raghunathan"], "title": "Jailbreaking in the Haystack", "comment": null, "summary": "Recent advances in long-context language models (LMs) have enabled\nmillion-token inputs, expanding their capabilities across complex tasks like\ncomputer-use agents. Yet, the safety implications of these extended contexts\nremain unclear. To bridge this gap, we introduce NINJA (short for\nNeedle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by\nappending benign, model-generated content to harmful user goals. Critical to\nour method is the observation that the position of harmful goals play an\nimportant role in safety. Experiments on standard safety benchmark, HarmBench,\nshow that NINJA significantly increases attack success rates across\nstate-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,\nand Gemini. Unlike prior jailbreaking methods, our approach is low-resource,\ntransferable, and less detectable. Moreover, we show that NINJA is\ncompute-optimal -- under a fixed compute budget, increasing context length can\noutperform increasing the number of trials in best-of-N jailbreak. These\nfindings reveal that even benign long contexts -- when crafted with careful\ngoal positioning -- introduce fundamental vulnerabilities in modern LMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04711", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04711", "abs": "https://arxiv.org/abs/2511.04711", "authors": ["Wenyuan Yang", "Yichen Sun", "Changzheng Chen", "Zhixuan Chu", "Jiaheng Zhang", "Yiming Li", "Dacheng Tao"], "title": "SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking", "comment": "The first two authors contributed equally to this work. 27 pages", "summary": "Large-scale vision-language models, especially CLIP, have demonstrated\nremarkable performance across diverse downstream tasks. Soft prompts, as\ncarefully crafted modules that efficiently adapt vision-language models to\nspecific tasks, necessitate effective copyright protection. In this paper, we\ninvestigate model copyright protection by auditing whether suspicious\nthird-party models incorporate protected soft prompts. While this can be viewed\nas a special case of model ownership auditing, our analysis shows that existing\ntechniques are ineffective due to prompt learning's unique characteristics.\nNon-intrusive auditing is inherently prone to false positives when independent\nmodels share similar data distributions with victim models. Intrusive\napproaches also fail: backdoor methods designed for CLIP cannot embed\nfunctional triggers, while extending traditional DNN backdoor techniques to\nprompt learning suffers from harmfulness and ambiguity challenges. We find that\nthese failures in intrusive auditing stem from the same fundamental reason:\nwatermarking operates within the same decision space as the primary task yet\npursues opposing objectives. Motivated by these findings, we propose sequential\nwatermarking for soft prompts (SWAP), which implants watermarks into a\ndifferent and more complex space. SWAP encodes watermarks through a specific\norder of defender-specified out-of-distribution classes, inspired by the\nzero-shot prediction capability of CLIP. This watermark, which is embedded in a\nmore complex space, keeps the original prediction label unchanged, making it\nless opposed to the primary task. We further design a hypothesis-test-guided\nverification protocol for SWAP and provide theoretical analyses of success\nconditions. Extensive experiments on 11 datasets demonstrate SWAP's\neffectiveness, harmlessness, and robustness against potential adaptive attacks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05182", "categories": ["cs.AI", "cs.CY", "H.4.2; I.2.3; I.2.6; I.2.8; J.7"], "pdf": "https://arxiv.org/pdf/2511.05182", "abs": "https://arxiv.org/abs/2511.05182", "authors": ["Johan Schubert", "Patrik Hansen", "Pontus H\u00f6rling", "Ronnie Johansson"], "title": "Autonomous generation of different courses of action in mechanized combat operations", "comment": "In Proceedings of the 30th International Command and Control Research\n  & Technology Symposium, Stockholm, Sweden, 3-6 November 2025, paper 009", "summary": "In this paper, we propose a methodology designed to support decision-making\nduring the execution phase of military ground combat operations, with a focus\non one's actions. This methodology generates and evaluates recommendations for\nvarious courses of action for a mechanized battalion, commencing with an\ninitial set assessed by their anticipated outcomes. It systematically produces\nthousands of individual action alternatives, followed by evaluations aimed at\nidentifying alternative courses of action with superior outcomes. These\nalternatives are appraised in light of the opponent's status and actions,\nconsidering unit composition, force ratios, types of offense and defense, and\nanticipated advance rates. Field manuals evaluate battle outcomes and\nadvancement rates. The processes of generation and evaluation work\nconcurrently, yielding a variety of alternative courses of action. This\napproach facilitates the management of new course generation based on\npreviously evaluated actions. As the combat unfolds and conditions evolve,\nrevised courses of action are formulated for the decision-maker within a\nsequential decision-making framework.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04716", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04716", "abs": "https://arxiv.org/abs/2511.04716", "authors": ["Mingliang Hou", "Yinuo Wang", "Teng Guo", "Zitao Liu", "Wenzhou Dou", "Jiaqi Zheng", "Renqiang Luo", "Mi Tian", "Weiqi Luo"], "title": "P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models", "comment": null, "summary": "Cognitive diagnosis models (CDMs) are pivotal for creating fine-grained\nlearner profiles in modern intelligent education platforms. However, these\nmodels are trained on sensitive student data, raising significant privacy\nconcerns. While membership inference attacks (MIA) have been studied in various\ndomains, their application to CDMs remains a critical research gap, leaving\ntheir privacy risks unquantified. This paper is the first to systematically\ninvestigate MIA against CDMs. We introduce a novel and realistic grey box\nthreat model that exploits the explainability features of these platforms,\nwhere a model's internal knowledge state vectors are exposed to users through\nvisualizations such as radar charts. We demonstrate that these vectors can be\naccurately reverse-engineered from such visualizations, creating a potent\nattack surface. Based on this threat model, we propose a profile-based MIA\n(P-MIA) framework that leverages both the model's final prediction\nprobabilities and the exposed internal knowledge state vectors as features.\nExtensive experiments on three real-world datasets against mainstream CDMs show\nthat our grey-box attack significantly outperforms standard black-box\nbaselines. Furthermore, we showcase the utility of P-MIA as an auditing tool by\nsuccessfully evaluating the efficacy of machine unlearning techniques and\nrevealing their limitations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05311", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05311", "abs": "https://arxiv.org/abs/2511.05311", "authors": ["Valeriu Dimidov", "Faisal Hawlader", "Sasan Jafarnejad", "Rapha\u00ebl Frank"], "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance", "comment": null, "summary": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04728", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04728", "abs": "https://arxiv.org/abs/2511.04728", "authors": ["Daniyal Ganiuly", "Assel Smaiyl"], "title": "Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models", "comment": "10 pages, 5 figures", "summary": "Phishing emails continue to pose a persistent challenge to online\ncommunication, exploiting human trust and evading automated filters through\nrealistic language and adaptive tactics. While large language models (LLMs)\nsuch as GPT-4 and LLaMA-3-8B achieve strong accuracy in text classification,\ntheir deployment in security systems requires assessing reliability beyond\nbenchmark performance. To address this, this study introduces the\nTrustworthiness Calibration Framework (TCF), a reproducible methodology for\nevaluating phishing detectors across three dimensions: calibration,\nconsistency, and robustness. These components are integrated into a bounded\nindex, the Trustworthiness Calibration Index (TCI), and complemented by the\nCross-Dataset Stability (CDS) metric that quantifies stability of\ntrustworthiness across datasets. Experiments conducted on five corpora, such as\nSecureMail 2025, Phishing Validation 2024, CSDMC2010, Enron-Spam, and Nazario,\nusing DeBERTa-v3-base, LLaMA-3-8B, and GPT-4 demonstrate that GPT-4 achieves\nthe strongest overall trust profile, followed by LLaMA-3-8B and\nDeBERTa-v3-base. Statistical analysis confirms that reliability varies\nindependently of raw accuracy, underscoring the importance of trust-aware\nevaluation for real-world deployment. The proposed framework establishes a\ntransparent and reproducible foundation for assessing model dependability in\nLLM-based phishing detection.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05375", "abs": "https://arxiv.org/abs/2511.05375", "authors": ["Sijie Yang", "Jiatong Li", "Filip Biljecki"], "title": "Reasoning Is All You Need for Urban Planning AI", "comment": "Submitted to AAAI 2026 Workshop AI4UP", "summary": "AI has proven highly successful at urban planning analysis -- learning\npatterns from data to predict future conditions. The next frontier is\nAI-assisted decision-making: agents that recommend sites, allocate resources,\nand evaluate trade-offs while reasoning transparently about constraints and\nstakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting,\nReAct, and multi-agent collaboration frameworks -- now make this vision\nachievable.\n  This position paper presents the Agentic Urban Planning AI Framework for\nreasoning-capable planning agents that integrates three cognitive layers\n(Perception, Foundation, Reasoning) with six logic components (Analysis,\nGeneration, Verification, Evaluation, Collaboration, Decision) through a\nmulti-agents collaboration framework. We demonstrate why planning decisions\nrequire explicit reasoning capabilities that are value-based (applying\nnormative principles), rule-grounded (guaranteeing constraint satisfaction),\nand explainable (generating transparent justifications) -- requirements that\nstatistical learning alone cannot fulfill. We compare reasoning agents with\nstatistical learning, present a comprehensive architecture with benchmark\nevaluation metrics, and outline critical research challenges. This framework\nshows how AI agents can augment human planners by systematically exploring\nsolution spaces, verifying regulatory compliance, and deliberating over\ntrade-offs transparently -- not replacing human judgment but amplifying it with\ncomputational reasoning capabilities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04860", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.04860", "abs": "https://arxiv.org/abs/2511.04860", "authors": ["Reworr", "Artem Petrov", "Dmitrii Volkov"], "title": "GPT-5 at CTFs: Case Studies From Top-Tier Cybersecurity Events", "comment": null, "summary": "OpenAI and DeepMind's AIs recently got gold at the IMO math olympiad and ICPC\nprogramming competition. We show frontier AI is similarly good at hacking by\nletting GPT-5 compete in elite CTF cybersecurity competitions.\n  In one of this year's hardest events, it outperformed 93% of humans finishing\n25th: between the world's #3-ranked team (24th place) and #7-ranked team (26th\nplace).\n  This report walks through our methodology, results, and their implications,\nand dives deep into 3 problems and solutions we found particularly interesting.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04882", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.04882", "abs": "https://arxiv.org/abs/2511.04882", "authors": ["Joon Kim", "Chengwei Duan", "Sandip Ray"], "title": "Bit-Flipping Attack Exploration and Countermeasure in 5G Network", "comment": "Presented at the IEEE MASS 2025 REUNS Workshop", "summary": "5G communication technology has become a vital component in a wide range of\napplications due to its unique advantages such as high data rate and low\nlatency. While much of the existing research has focused on optimizing its\nefficiency and performance, security considerations have not received\ncomparable attention, potentially leaving critical vulnerabilities unexplored.\nIn this work, we investigate the vulnerability of 5G systems to bit-flipping\nattacks, which is an integrity attack where an adversary intercepts 5G network\ntraffic and modifies specific fields of an encrypted message without\ndecryption, thus mutating the message while remaining valid to the receiver.\nNotably, these attacks do not require the attacker to know the plaintext, and\nonly the semantic meaning or position of certain fields would be enough to\neffect targeted modifications. We conduct our analysis on OpenAirInterface\n(OAI), an open-source 5G platform that follows the 3GPP Technical\nSpecifications, to rigorously test the real-world feasibility and impact of\nbit-flipping attacks under current 5G encryption mechanisms. Finally, we\npropose a keystream-based shuffling defense mechanism to mitigate the effect of\nsuch attacks by raising the difficulty of manipulating specific encrypted\nfields, while introducing no additional communication overhead compared to the\nNAS Integrity Algorithm (NIA) in 5G. Our findings reveal that enhancements to\n5G security are needed to better protect against attacks that alter data during\ntransmission at the network level.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04925", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.04925", "abs": "https://arxiv.org/abs/2511.04925", "authors": ["Rethish Nair Rajendran", "Sathish Krishna Anumula", "Dileep Kumar Rai", "Sachin Agrawal"], "title": "Zero Trust Security Model Implementation in Microservices Architectures Using Identity Federation", "comment": null, "summary": "The microservice bombshells that have been linked with the microservice\nexpansion have altered the application architectures, offered agility and\nscalability in terms of complexity in security trade-offs. Feeble legacy-based\nperimeter-based policies are unable to offer safeguard to distributed workloads\nand temporary interaction among and in between the services. The article itself\nis a case on the need of the Zero Trust Security Model of micro services\necosystem, particularly, the fact that human and workloads require identity\nfederation. It is proposed that the solution framework will be based on\nindustry-standard authentication and authorization and end-to-end trust\nidentity technologies, including Authorization and OpenID connect (OIDC),\nAuthorization and OAuth 2.0 token exchange, and Authorization and SPIFFE/ SPIRE\nworkload identities. Experimental evaluation is a unique demonstration of a\nsuperior security position of making use of a smaller attack surface, harmony\npolicy enforcement, as well as interoperability across multi- domain\nenvironments. The research results overlay that the federated identity combined\nwith the Zero Trust basics not only guarantee the rules relating to\nauthentication and authorization but also fully complies with the latest\nDevSecOps standards of microservice deployment, which is automated, scaled, and\nresilient. The current project offers a stringent roadmap to the organizations\nthat desire to apply Zero Trust in cloud-native technologies but will as well\nguarantee adherence and interoperability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.04946", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04946", "abs": "https://arxiv.org/abs/2511.04946", "authors": ["Lei Chen", "Erci Xu", "Yiming Sun", "Shengyu Fan", "Xianglong Deng", "Guiming Shi", "Guang Fan", "Liang Kong", "Yilan Zhu", "Shoumeng Yan", "Mingzhe Zhang"], "title": "The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective", "comment": "https://link.springer.com/chapter/10.1007/978-981-95-1021-4_25", "summary": "Fully Homomorphic Encryption (FHE) allows computations to be performed on\nencrypted data, significantly enhancing user privacy. However, the I/O\nchallenges associated with deploying FHE applications remains understudied. We\nanalyze the impact of storage I/O on the performance of FHE applications and\nsummarize key lessons from the status quo. Key results include that storage I/O\ncan degrade the performance of ASICs by as much as 357$\\times$ and reduce GPUs\nperformance by up to 22$\\times$.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05097", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05097", "abs": "https://arxiv.org/abs/2511.05097", "authors": ["Romain Lefeuvre", "Charly Reux", "Stefano Zacchiroli", "Olivier Barais", "Benoit Combemale"], "title": "Chasing One-day Vulnerabilities Across Open Source Forks", "comment": null, "summary": "Tracking vulnerabilities inherited from third-party open-source components is\na well-known challenge, often addressed by tracing the threads of dependency\ninformation. However, vulnerabilities can also propagate through forking: a\nrepository forked after the introduction of a vulnerability, but before it is\npatched, may remain vulnerable in the fork well after being fixed in the\noriginal project. Current approaches for vulnerability analysis lack the\ncommit-level granularity needed to track vulnerability introductions and fixes\nacross forks, potentially leaving one-day vulnerabilities undetected. This\npaper presents a novel approach to help developers identify one-day\nvulnerabilities in forked repositories. Leveraging the global graph of public\ncode, as captured by the Software Heritage archive, the approach propagates\nvulnerability information at the commit level and performs automated impact\nanalysis. This enables automatic detection of forked projects that have not\nincorporated fixes, leaving them potentially vulnerable. Starting from 7162\nrepositories that, according to OSV, include vulnerable commits in their\ndevelopment histories, we identify 2.2 M forks, containing at least one\nvulnerable commit. Then we perform a strict filtering, allowing us to find 356\n___vulnerability, fork___ pairs impacting active and popular GitHub forks, we\nmanually evaluate 65 pairs, finding 3 high-severity vulnerabilities,\ndemonstrating the impact and applicability of this approach.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05156", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI", "C.2.3"], "pdf": "https://arxiv.org/pdf/2511.05156", "abs": "https://arxiv.org/abs/2511.05156", "authors": ["Azhar Hussain Mozumder", "M. John Basha", "Chayapathi A. R"], "title": "SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks", "comment": "20 pages, 12 figures", "summary": "With more and more existing networks being transformed to Software-Defined\nNetworking (SDN), they need to be more secure and demand smarter ways of\ntraffic control. This work, SmartSecChain-SDN, is a platform that combines\nmachine learning based intrusion detection, blockchain-based storage of logs,\nand application-awareness-based priority in SDN networks. To detect network\nintrusions in a real-time, precision and low-false positives setup, the\nframework utilizes the application of advanced machine learning algorithms,\nnamely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is\nbased on the Hyperledger Fabric, which is a permissioned blockchain technology,\nto provide secure, scalable, and privacy-preserving storage and, thus,\nguarantee that the Intrusion Detection System (IDS) records cannot be altered\nand can be analyzed comprehensively. The system also has Quality of Service\n(QoS) rules and traffic shaping based on applications, which enables\nprioritization of critical services, such as VoIP, video conferencing, and\nbusiness applications, as well as de-prioritization of non-essential traffic,\nsuch as downloads and updates. Mininet can simulate real-time SDN scenarios\nbecause it is used to prototype whole architectures. It is also compatible with\ncontrollers OpenDaylight and Ryu. It has tested the framework using the InSDN\ndataset and proved that it can identify different kinds of cyberattacks and\nhandle bandwidth allocation efficiently under circumstances of resource\nconstraints. SmartSecChain-SDN comprehensively addresses SDN system protection,\nsecuring and enhancing. The proposed study offers an innovative, extensible way\nto improve cybersecurity, regulatory compliance, and the administration of\nnext-generation programmable networks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05100", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05100", "abs": "https://arxiv.org/abs/2511.05100", "authors": ["Arslan Mumtaz", "Mridula Singh"], "title": "TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS", "comment": null, "summary": "Global Navigation Satellite Systems (GNSS) provide Positioning, Navigation,\nand Timing (PNT) information to over 4 billion devices worldwide. Despite its\npervasive use in safety critical and high precision applications, GNSS remains\nvulnerable to spoofing attacks. Cryptographic enhancements, such as the use of\nTESLA protocol in Galileo, to provide navigation message authentication do not\nmitigate time of arrival manipulations. In this paper, we propose TRICK, a\nprimitive for secure positioning that closes this gap by introducing a\nfundamentally new approach that only requires two way communications with a\nsingle reference node along with multiple broadcast signals. Unlike classical\nVerifiable Multilateration (VM), which requires establishing two way\ncommunication with each reference nodes, our solution relies on only two\nmeasurements with a trusted Low Earth Orbiting (LEO) satellite and combines\nbroadcast navigation signals. We rigorously prove that combining the LEO\nsatellite based two way range measurements and multiple one way ranges such as\nfrom broadcast signals of GNSS into ellipsoidal constraint restores the same\nguarantees as offered by VM whilst using minimal infrastructure and message\nexchanges. Through detailed analysis, we show that our approach reliably\ndetects spoofing attempts while adding negligible computation overhead.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05102", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05102", "abs": "https://arxiv.org/abs/2511.05102", "authors": ["Disesdi Susanna Cox", "Niklas Bunzel"], "title": "Quantifying the Risk of Transferred Black Box Attacks", "comment": null, "summary": "Neural networks have become pervasive across various applications, including\nsecurity-related products. However, their widespread adoption has heightened\nconcerns regarding vulnerability to adversarial attacks. With emerging\nregulations and standards emphasizing security, organizations must reliably\nquantify risks associated with these attacks, particularly regarding\ntransferred adversarial attacks, which remain challenging to evaluate\naccurately. This paper investigates the complexities involved in resilience\ntesting against transferred adversarial attacks. Our analysis specifically\naddresses black-box evasion attacks, highlighting transfer-based attacks due to\ntheir practical significance and typically high transferability between neural\nnetwork models. We underline the computational infeasibility of exhaustively\nexploring high-dimensional input spaces to achieve complete test coverage. As a\nresult, comprehensive adversarial risk mapping is deemed impractical. To\nmitigate this limitation, we propose a targeted resilience testing framework\nthat employs surrogate models strategically selected based on Centered Kernel\nAlignment (CKA) similarity. By leveraging surrogate models exhibiting both high\nand low CKA similarities relative to the target model, the proposed approach\nseeks to optimize coverage of adversarial subspaces. Risk estimation is\nconducted using regression-based estimators, providing organizations with\nrealistic and actionable risk quantification.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05110", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.05110", "abs": "https://arxiv.org/abs/2511.05110", "authors": ["Xingzhi Zhang", "Buyi Lv", "Yimin Lu", "Kai Bu"], "title": "PhantomFetch: Obfuscating Loads against Prefetcher Side-Channel Attacks", "comment": null, "summary": "The IP-stride prefetcher has recently been exploited to leak secrets through\nside-channel attacks. It, however, cannot be simply disabled for security with\nprefetching speedup as a sacrifice. The state-of-the-art defense tries to\nretain the prefetching effect by hardware modification. In this paper, we\npresent PhantomFetch as the first prefetching-retentive and hardware-agnostic\ndefense. It avoids potential remanufacturing cost and enriches applicability to\noff-the-shelf devices. The key idea is to directly break the exploitable\ncoupling between trained prefetcher entries and the victim's secret-dependent\nloads by obfuscating the sensitive load effects of the victim. The experiment\nresults show that PhantomFetch can secure the IP-stride prefetcher with only\nnegligible overhead.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05111", "categories": ["cs.CR", "cs.IT", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.05111", "abs": "https://arxiv.org/abs/2511.05111", "authors": ["Do Hyun Kim", "Ahmet Cetinkaya"], "title": "Confidentiality in a Card-Based Protocol Under Repeated Biased Shuffles", "comment": "17 pages, 2 figures", "summary": "In this paper, we provide a probabilistic analysis of the confidentiality in\na card-based protocol. We focus on Bert den Boer's original Five Card Trick to\ndevelop our approach. Five Card Trick was formulated as a secure two-party\ncomputation method, where two players use colored cards with identical backs to\ncalculate the logical AND operation on the bits that they choose. In this\nmethod, the players first arrange the cards privately, and then shuffle them\nthrough a random cut. Finally, they reveal the shuffled arrangement to\ndetermine the result of the operation. An unbiased random cut is essential to\nprevent players from exposing their chosen bits to each other. However, players\ntypically choose to move cards within the deck even though not moving any cards\nshould be equally likely. This unconscious behavior results in a biased,\nnonuniform shuffling-distribution in the sense that some arrangements of cards\nare slightly more probable after the cut. Such a nonuniform distribution\ncreates an opportunity for a malicious player to gain advantage in guessing the\nother player's choice. We provide the conditional probabilities of such guesses\nas a way to quantify the information leakage. Furthermore, we utilize the\neigenstructure of a Markov chain to derive tight bounds on the number of times\nthe biased random cuts must be repeated to reduce the leakage to an acceptable\nlevel. We also discuss the generalization of our approach to the setting where\nshuffling is conducted by a malicious player.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05119", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05119", "abs": "https://arxiv.org/abs/2511.05119", "authors": ["V\u00edctor Mayoral-Vilches", "Luis Javier Navarrete-Lozano", "Francesco Balassone", "Mar\u00eda Sanz-G\u00f3mez", "Crist\u00f3bal Ricardo Veas Ch\u00e1vez", "Maite del Mundo de Torres"], "title": "Cybersecurity AI in OT: Insights from an AI Top-10 Ranker in the Dragos OT CTF 2025", "comment": null, "summary": "Operational Technology (OT) cybersecurity increasingly relies on rapid\nresponse across malware analysis, network forensics, and reverse engineering\ndisciplines. We examine the performance of Cybersecurity AI (CAI), powered by\nthe \\texttt{alias1} model, during the Dragos OT CTF 2025 -- a 48-hour\nindustrial control system (ICS) competition with more than 1,000 teams. Using\nCAI telemetry and official leaderboard data, we quantify CAI's trajectory\nrelative to the leading human-operated teams. CAI reached Rank~1 between\ncompetition hours 7.0 and 8.0, crossed 10,000 points at 5.42~hours\n(1,846~pts/h), and completed 32 of the competition's 34 challenges before\nautomated operations were paused at hour~24 with a final score of 18,900 points\n(6th place). The top-3 human teams solved 33 of 34 challenges, collectively\nleaving only the 600-point ``Kiddy Tags -- 1'' unsolved; they were also the\nonly teams to clear the 1,000-point ``Moot Force'' binary. The top-5 human\nteams averaged 1,347~pts/h to the same milestone, marking a 37\\% velocity\nadvantage for CAI. We analyse time-resolved scoring, category coverage, and\nsolve cadence. The evidence indicates that a mission-configured AI agent can\nmatch or exceed expert human crews in early-phase OT incident response while\nremaining subject to practical limits in sustained, multi-day operations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05133", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.05133", "abs": "https://arxiv.org/abs/2511.05133", "authors": ["Urslla Uchechi Izuazu", "Mounir Bensalem", "Admela Jukan"], "title": "A Secured Intent-Based Networking (sIBN) with Data-Driven Time-Aware Intrusion Detection", "comment": "This paper is uploaded here for research community, thus it is for\n  non-commercial purposes", "summary": "While Intent-Based Networking (IBN) promises operational efficiency through\nautonomous and abstraction-driven network management, a critical unaddressed\nissue lies in IBN's implicit trust in the integrity of intent ingested by the\nnetwork. This inherent assumption of data reliability creates a blind spot\nexploitable by Man-in-the-Middle (MitM) attacks, where an adversary intercepts\nand alters intent before it is enacted, compelling the network to orchestrate\nmalicious configurations. This study proposes a secured IBN (sIBN) system with\ndata driven intrusion detection method designed to secure legitimate user\nintent from adversarial tampering. The proposed intent intrusion detection\nsystem uses a ML model applied for network behavioral anomaly detection to\nreveal temporal patterns of intent tampering. This is achieved by leveraging a\nset of original behavioral metrics and newly engineered time-aware features,\nwith the model's hyperparameters fine-tuned through the randomized search\ncross-validation (RSCV) technique. Numerical results based on real-world data\nsets, show the effectiveness of sIBN, achieving the best performance across\nstandard evaluation metrics, in both binary and multi classification tasks,\nwhile maintaining low error rates.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05193", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05193", "abs": "https://arxiv.org/abs/2511.05193", "authors": ["Zhibo Dong", "Yong Huang", "Shubao Sun", "Wentao Cui", "Zhihua Wang"], "title": "BLADE: Behavior-Level Anomaly Detection Using Network Traffic in Web Services", "comment": "Accepted by IEEE MSN 2025", "summary": "With their widespread popularity, web services have become the main targets\nof various cyberattacks. Existing traffic anomaly detection approaches focus on\nflow-level attacks, yet fail to recognize behavior-level attacks, which appear\nbenign in individual flows but reveal malicious purpose using multiple network\nflows. To transcend this limitation, we propose a novel unsupervised traffic\nanomaly detection system, BLADE, capable of detecting not only flow-level but\nalso behavior-level attacks in web services. Our key observation is that\napplication-layer operations of web services exhibit distinctive communication\npatterns at the network layer from a multi-flow perspective. BLADE first\nexploits a flow autoencoder to learn a latent feature representation and\ncalculates its reconstruction losses per flow. Then, the latent representation\nis assigned a pseudo operation label using an unsupervised clustering method.\nNext, an anomaly score is computed based on the reconstruction losses. Finally,\nthe triplets of timestamps, pseudo labels, and anomaly scores from multiple\nflows are aggregated and fed into a one-class classifier to characterize the\nbehavior patterns of legitimate web operations, enabling the detection of\nflow-level and behavior-level anomalies. BLADE is extensively evaluated on both\nthe custom dataset and the CIC-IDS2017 dataset. The experimental results\ndemonstrate BLADE's superior performance, achieving high F1 scores of 0.9732\nand 0.9801, respectively, on the two datasets, and outperforming traditional\nsingle-flow anomaly detection baselines.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.05359", "categories": ["cs.CR", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.05359", "abs": "https://arxiv.org/abs/2511.05359", "authors": ["Amr Gomaa", "Ahmed Salem", "Sahar Abdelnabi"], "title": "ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations", "comment": null, "summary": "As language models evolve into autonomous agents that act and communicate on\nbehalf of users, ensuring safety in multi-agent ecosystems becomes a central\nchallenge. Interactions between personal assistants and external service\nproviders expose a core tension between utility and protection: effective\ncollaboration requires information sharing, yet every exchange creates new\nattack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating\nprivacy and security risks in agent-agent interactions. ConVerse spans three\npractical domains (travel, real estate, insurance) with 12 user personas and\nover 864 contextually grounded attacks (611 privacy, 253 security). Unlike\nprior single-agent settings, it models autonomous, multi-turn agent-to-agent\nconversations where malicious requests are embedded within plausible discourse.\nPrivacy is tested through a three-tier taxonomy assessing abstraction quality,\nwhile security attacks target tool use and preference manipulation. Evaluating\nseven state-of-the-art models reveals persistent vulnerabilities; privacy\nattacks succeed in up to 88% of cases and security breaches in up to 60%, with\nstronger models leaking more. By unifying privacy and security within\ninteractive multi-agent contexts, ConVerse reframes safety as an emergent\nproperty of communication.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
